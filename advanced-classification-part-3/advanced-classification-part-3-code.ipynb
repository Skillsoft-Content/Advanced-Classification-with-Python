{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## ADVANCED CLASSIFICATION PART 3/ADVANCED CLASSIFICATION PART 3 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20744948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 2: Loading packages  ####\n",
    "\n",
    "# Helper packages.\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt                     \n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "# Scikit-learn packages for building models and model evaluation.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 7: Directory settings  ####\n",
    "\n",
    "# Set 'main_dir' to location of the project folder\n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 8: Load the cleaned dataset  ####\n",
    "\n",
    "costa_clean = pickle.load(open(\"costa_clean.sav\",\"rb\"))\n",
    "print(costa_clean.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 9: Print info for our data  ####\n",
    "\n",
    "costa_clean.columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083bbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 10: Split into training and test sets  ####\n",
    "\n",
    "# Select the predictors and target.\n",
    "X = costa_clean.drop(['Target'], axis = 1)\n",
    "y = np.array(costa_clean['Target'])\n",
    "\n",
    "# Set the seed to 1.\n",
    "np.random.seed(1)\n",
    "\n",
    "# Split into training and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd8fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 11: Recap: fit vanilla RF model  ####\n",
    "\n",
    "# Initialize the classifier.\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "# Fit training data.\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 12: Recap: predict using vanilla RF model  ####\n",
    "\n",
    "# Predict on test.\n",
    "forest_y_predict = forest.predict(X_test)\n",
    "print(forest_y_predict[:5])\n",
    "#Predict on test, but instead of labels \n",
    "# we will get probabilities for class 0 and 1.\n",
    "forest_y_predict_prob = forest.predict_proba(X_test) \n",
    "print(forest_y_predict_prob[5:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12debdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 13: Recap: confusion matrix and accuracy  ####\n",
    "\n",
    "# Compute accuracy for RF model.\n",
    "forest_accuracy = metrics.accuracy_score(y_test, \n",
    "                                         forest_y_predict)\n",
    "print(forest_accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40dbd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 15: Precision  ####\n",
    "\n",
    "forest_precision = metrics.precision_score(y_test, \n",
    "                                           forest_y_predict)\n",
    "print(forest_precision)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e20a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 16: Recall  ####\n",
    "\n",
    "forest_recall = metrics.recall_score(y_test, \n",
    "                                     forest_y_predict)\n",
    "print(forest_recall)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d51b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 18: Precision-recall curve: the visual of tradeoff  ####\n",
    "\n",
    "rf_prec_recall = metrics.plot_precision_recall_curve(forest, X_test, y_test, name = \"RF\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee99e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 19: F1: precision vs recall   ####\n",
    "\n",
    "forest_f1 = metrics.f1_score(y_test, \n",
    "                             forest_y_predict)\n",
    "print(forest_f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962dbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 20: Fbeta: precision vs recall (weighted)  ####\n",
    "\n",
    "forest_fbeta = metrics.fbeta_score(y_test, \n",
    "                                   forest_y_predict, \n",
    "                                   beta = 0.5)\n",
    "print(forest_fbeta)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 24: Log loss using scikit-learn metrics  ####\n",
    "\n",
    "# The second argument is an array of predicted probabilities, not labels!\n",
    "forest_log_loss = metrics.log_loss(y_test, forest_y_predict_prob[:, 1], eps=1e-15)\n",
    "print (\"Log loss: \", forest_log_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 25: Log loss using scikit-learn metrics - cont'd  ####\n",
    "\n",
    "# Convert a difficult to interpret log loss to an overall accuracy in predicted probabilities.\n",
    "print(\"Overall accuracy: \", math.exp(-forest_log_loss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 26: Loss curve: ideal case  ####\n",
    "\n",
    "# Probability values: 0 to 1 in 0.01 increments.\n",
    "prob_increments = [x*0.01 for x in range(0, 101)]\n",
    "\n",
    "# Evaluate predictions for when true value is 0.\n",
    "loss_0 = [metrics.log_loss([0], [x], labels=[0,1]) for x in prob_increments]\n",
    "\n",
    "# Evaluate predictions for when true value is 1.\n",
    "loss_1 = [metrics.log_loss([1], [x], labels=[0,1]) for x in prob_increments]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 27: Loss curve: ideal case (cont'd)  ####\n",
    "\n",
    "# Plot probability increments vs loss curves.\n",
    "plt.plot(prob_increments, loss_0, label='true = 0')\n",
    "plt.plot(prob_increments, loss_1, label='true = 1')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 28: Loss curve: based on our data  ####\n",
    "\n",
    "# Loss for predicting different fixed probability values.\n",
    "losses = [metrics.log_loss(y_test, [y for x in range(len(y_test))]) for y in prob_increments]\n",
    "# Plot predictions vs loss.\n",
    "plt.plot(prob_increments, losses)\n",
    "plt.show()\n",
    "# Loss for predicting different fixed probability values.\n",
    "losses = [metrics.log_loss(y_test, [y for x in range(len(y_test))]) for y in prob_increments]\n",
    "# Plot predictions vs loss.\n",
    "plt.plot(prob_increments, losses)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 31: ROC curve: the tradeoff  ####\n",
    "\n",
    "rf_roc = metrics.plot_roc_curve(forest, X_test, y_test, name = \"RF\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 32: AUC: area under the ROC curve  ####\n",
    "\n",
    "# Where y_pred are probabilities and y_true are binary class labels\n",
    "forest_auc = metrics.roc_auc_score(y_test, forest_y_predict_prob[:, 1])\n",
    "print(\"AUC: \", forest_auc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410799f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 33: Wrapping score evaluation into function  ####\n",
    "\n",
    "def get_performance_scores(y_test, y_predict, y_predict_prob, eps=1e-15, beta=0.5):\n",
    "    from sklearn import metrics\n",
    "    # Scores keys.\n",
    "    metric_keys = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"fbeta\", \"log_loss\", \"AUC\"]\n",
    "    # Score values.\n",
    "    metric_values = [None]*len(metric_keys)\n",
    "    metric_values[0] = metrics.accuracy_score(y_test, y_predict)\n",
    "    metric_values[1] = metrics.precision_score(y_test, y_predict)\n",
    "    metric_values[2] = metrics.recall_score(y_test, y_predict)\n",
    "    metric_values[3] = metrics.f1_score(y_test, y_predict)\n",
    "    metric_values[4] = metrics.fbeta_score(y_test, y_predict, beta=beta)\n",
    "    metric_values[5] = metrics.log_loss(y_test, y_predict_prob[:, 1], eps=eps)\n",
    "    metric_values[6] = metrics.roc_auc_score(y_test, y_predict_prob[:, 1])\n",
    "    perf_metrics = dict(zip(metric_keys, metric_values))\n",
    "    return(perf_metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9de37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 34: Test score generating function  ####\n",
    "\n",
    "forest_scores = get_performance_scores(y_test, forest_y_predict, forest_y_predict_prob)\n",
    "\n",
    "metrics_forest = {\"RF\": forest_scores}\n",
    "print(metrics_forest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 36: Exercise 1  ####\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251350a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 38: Vanilla RF model parameters  ####\n",
    "\n",
    "forest.get_params()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 41: Parameter grid  ####\n",
    "\n",
    "# Number of trees in random forest.\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 20)]\n",
    "\n",
    "# Number of features to consider at every split.\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree.\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node.\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node.\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Set Minimal Cost-Complexity Pruning parameter (has to be >= 0.0).\n",
    "ccp_alpha = [0.0, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Create the random grid \n",
    "# (a python dictionary in a form `'parameter_name': parameter_values`)\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'ccp_alpha': ccp_alpha}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 42: Set up RandomizedSearchCV function  ####\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = forest, #<- model object\n",
    "                               param_distributions = random_grid, #<- param grid\n",
    "                               n_iter = 100,#<- number of param. settings sampled \n",
    "                               cv = 3,      #<- 3-fold CV\n",
    "                               verbose = 0, #<- silence lengthy output to console\n",
    "                               random_state = 1, #<- set random state\n",
    "                               n_jobs = -1)      #<- use all available processors\n",
    "# Fit the random search model.\n",
    "rf_random.fit(X_train, y_train) #<- fit like any other scikit-learn model\n",
    "# Take a look at optimal combination of parameters.\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 44: Optimized RF model  ####\n",
    "\n",
    "# Pass best parameters obtained through randomized search to RF classifier.\n",
    "optimized_forest = RandomForestClassifier(**rf_random.best_params_)\n",
    "\n",
    "# Train the optimized RF model.\n",
    "optimized_forest.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 45: Predict and compute performance scores  ####\n",
    "\n",
    "# Get predicted labels for test data.\n",
    "optimized_forest_y_predict = optimized_forest.predict(X_test)\n",
    "\n",
    "# Get predicted probabilities.\n",
    "optimized_forest_y_predict_proba = optimized_forest.predict_proba(X_test)\n",
    "# Compute performance scores.\n",
    "optimized_forest_scores = get_performance_scores(y_test, \n",
    "                                                 optimized_forest_y_predict,\n",
    "                                                 optimized_forest_y_predict_proba)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 46: Precision vs recall curve: the tradeoff  ####\n",
    "\n",
    "ax = plt.gca() #<- create a new axis object\n",
    "opt_rf_prec_recall = metrics.plot_precision_recall_curve(optimized_forest,\n",
    "                                 X_test, \n",
    "                                 y_test,\n",
    "                                 ax = ax,\n",
    "                                 name = \"Optimized RF\")\n",
    "rf_prec_recall.plot(ax = ax, name = \"RF\") #<- add rf plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 47: ROC curve: the tradeoff  ####\n",
    "\n",
    "ax = plt.gca()\n",
    "opt_rf_roc = metrics.plot_roc_curve(optimized_forest,\n",
    "                       X_test,\n",
    "                       y_test,\n",
    "                       name = \"Optimized RF\",\n",
    "                       ax = ax)\n",
    "\n",
    "rf_roc.plot(ax = ax, name = \"RF\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e1824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 48: Optimized RF: append to metrics dictionary  ####\n",
    "\n",
    "metrics_forest.update({\"Optimized RF\": optimized_forest_scores})\n",
    "print(metrics_forest)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
